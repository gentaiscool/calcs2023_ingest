- abstract: 
    'This paper contributes to German-English code-switching research. We provide the 
    largest corpus of naturally occurring German-English code-switching, 
    where English is included in German text, and two methods for code-switching 
    identification. The first method is rule-based, using wordlists and morphological 
    processing. We use this method to compile a corpus of 25.6M tweets employing 
    German-English code-switching. In our second method, we continue pretraining 
    of a neural language model on this corpus and classify tokens based on embeddings 
    from this language model. Our systems establish SoTA on our new corpus and an 
    existing German-English code-switching benchmark. In particular, we systematically 
    study code-switching for language-ambiguous words which can only be resolved 
    in context, and morphologically mixed words consisting of both English and 
    German morphemes. We distribute both corpora and systems to the research community.'
  attributes:
    paper_type: long
    presentation_type: oral
    submitted_area: Area 1
  authors:
  - email: is473@cam.ac.uk
    first_name: Igor
    instituion: University of Cambridge
    last_name: Sterner
    openreview: ~Igor_Sterner1
  - email: sht25@cam.ac.uk
    first_name: Simone
    instituion: University of Cambridge
    last_name: Teufel
    openreview: ~Simone_Teufel1 
  file: paper2.pdf
  id: 2
  title: 'TongueSwitcher: Fine-Grained Identification of German-English Code-Switching'
- abstract:
    'Code-switching (CS), i.e. mixing different languages in a single sentence, 
    is a common phenomenon in communication and can be challenging in many Natural 
    Language Processing (NLP) settings. Previous studies on CS speech have shown 
    promising results for end-to-end speech translation (ST), but have been limited 
    to offline scenarios and to translation to one of the languages present in the 
    source monolingual transcription). In this paper, we focus on two essential yet 
    unexplored areas for real-world CS speech translation: streaming settings, and 
    translation to a third language (i.e., a language not included in the source). 
    To this end, we extend the Fisher and Miami test and validation datasets to 
    include new targets in Spanish and German. Using this data, we train a model 
    for both offline and streaming ST and we establish baseline results for the 
    two settings mentioned earlier.'
  attributes:
    paper_type: long
    presentation_type: oral
    submitted_area: Area 1
  authors:
  - email: belen.alastruey@upc.edu
    first_name: Belen
    institution: TALP Research Center, Universitat Politècnica de Catalunya
    last_name: Alastruey
    openreview: ~Belen_Alastruey1
  - email: sperber@apple.com
    first_name: Matthias
    institution: Apple
    last_name: Sperber
    openreview: ~Matthias_Sperber2
  - email: sperber@apple.com
    first_name: Christian
    institution: Apple
    last_name: Gollan
    openreview: ~Christian_Gollan1
  - first_name: Dominic 
    institution: Apple
    last_name: Telaar
    openreview: ~Dominic_C_Telaar1
  - first_name: Tim 
    institution: Apple
    last_name: Ng
    openreview: ~Tim_Ng1
  - first_name: Aashish
    institution: Universität Duisburg-Essen
    last_name: Agarwal
    openreview: ~Aashish_Agarwal1
  file: paper3.pdf
  id: 3
  archival: True
  title: Towards Real-World Streaming Speech Translation for Code-Switched Speech
- abstract: 'Nepali-English code-switching (CS) has been a growing phenomenon in Nepalese 
    society, especially in social media. The code-switching text can be leveraged to 
    understand the socio-linguistic behaviours of the multilingual speakers. Existing 
    studies have attempted to identify the language preference of the multilingual 
    speakers for expressing different emotions using text in different language pairs. 
    In this work, we aim to study the language preference of multilingual Nepali-English 
    CS speakers while expressing sentiment in social media. We create a novel dataset 
    for sentiment analysis using the public Nepali-English code-switched comments 
    in YouTube. After performing the statistical study on the dataset, we find that 
    the proportion of use of Nepali language is higher in negative comments when 
    compared with positive comments, hence concluding the preference for using 
    native language while expressing negative sentiment. Machine learning and 
    transformer-based models are used as the baseline models for the dataset 
    for sentiment classification. The dataset is released publicly.'
  attributes:
    paper_type: long
    presentation_type: oral
    submitted_area: Area 1
  authors:
  - first_name: Niraj
    last_name: Pahari
  - first_name: Kazutaka
    last_name: Shimada
  file: paper4.pdf
  id: 4
  archival: True
  title: Language Preference for Expression of Sentiment for Nepali-English Bilingual 
    Speakers on Social Media
- abstract: 'Recognizing code-switching (CS) speech often presents challenges for an 
    automatic speech recognition system (ASR) due to limited linguistic context in 
    short monolingual segments, resulting in language confusion. To mitigate this issue, 
    language identity (LID) is often integrated into the speech recognition system to 
    provide additional linguistic context. However, previous works predominately focus 
    on extracting language identity from speech signals. We introduce a novel approach 
    to learn language identity from pure text data via a dedicated language 
    identity-language model. Besides, we explore two strategies: LID state fusion and 
    language posterior biasing, to integrate the text-derived language identities into 
    the end-to-end ASR system. By incorporating hypothesized language identities, our 
    ASR system gains crucial contextual cues, effectively capturing language 
    transitions and patterns within code-switched utterances. We conduct speech 
    recognition experiments on the SEAME corpus and demonstrate the effectiveness 
    of our proposed methods. Our results reveal significantly improved transcriptions 
    in code-switching scenarios, underscoring the potential of text-derived LID in 
    enhancing code-switching speech recognition.'
  attributes:
    paper_type: long
    presentation_type: oral
    submitted_area: Area 1
  authors:
  - first_name: Qinyi
    last_name: Wang
  - first_name: Haizhou
    last_name: Li
  file: paper5.pdf
  id: 5
  archival: True
  title: 'Text-Derived Language Identity Incorporation for End-to-End Code-Switching Speech Recognition'
- abstract: 'The differences in decision making between behavioural models of voice
    interfaces are hard to capture using existing measures for the absolute performance
    of such models. For instance, two models may have a similar task success rate,
    but very different ways of getting there. In this paper, we propose a general
    methodology to compute the similarity of two dialogue behaviour models and investigate
    different ways of computing scores on both the semantic and the textual level.
    Complementing absolute measures of performance, we test our scores on three different
    tasks and show the practical usability of the measures.'
  attributes:
    paper_type: long
    presentation_type: oral
    submitted_area: Area 1
  authors:
  - first_name: Zheng Xin
    last_name: Yong
    middle_name: Xin
  - first_name: Ruochen
    last_name: Zhang
  - first_name: Jessica
    last_name: Forde
    middle_name: Zosa
  - first_name: Skyler
    last_name: Wang
  - first_name: Arjun
    last_name: Subramonian
  - first_name: Holy
    last_name: Lovenia
  - first_name: Samuel
    last_name: Cahyawijaya
  - first_name: Genta
    last_name: Winata
    middle_name: Indra
  - first_name: Lintang
    last_name: Sutawika
  - first_name: Jan Christian Blaise
    last_name: Cruz
  - first_name: Yin Lin
    last_name: Tan
  - first_name: Long
    last_name: Phan
  - first_name: Long
    last_name: Phan
  - first_name: Rowena
    last_name: Garcia
  - first_name: Thamar
    last_name: Solorio
  - first_name: Alham
    last_name: Aji
    middle_name: Fikri
  file: paper6.pdf
  id: 6
  archival: True
  title: 'Prompting Multilingual Large Language Models to Generate Code-Mixed Texts: 
    The Case of South East Asian Languages'
- abstract: "The mixing of two or more languages is called Code-Mixing (CM). CM is a 
    social norm in multilingual societies. Neural Language Models (NLMs) like transformers 
    have been effective on many NLP tasks. However, NLM for CM is an under-explored area. 
    Though transformers are capable and powerful, they cannot always encode positional 
    information since they are non-recurrent. Therefore, to enrich word information and 
    incorporate positional information, positional encoding is defined. We hypothesize 
    that Switching Points (SPs), i.e., junctions in the text where the language 
    switches (L1 -> L2 or L2 -> L1), pose a challenge for CM Language Models (LMs), 
    and hence give special emphasis to SPs in the modeling process. We experiment 
    with several positional encoding mechanisms and show that rotatory positional 
    encodings along with switching point information yield the best results.

    We introduce CONFLATOR: a neural language modeling approach for code-mixed languages. 
    CONFLATOR tries to learn to emphasize switching points using smarter positional encoding, 
    both at unigram and bigram levels. CONFLATOR outperforms the state-of-the-art on two 
    tasks based on code-mixed Hindi and English (Hinglish): (i) sentiment analysis and 
    (ii) machine translation."
  attributes:
    paper_type: long
    presentation_type: oral
    submitted_area: Area 1
  authors:
  - first_name: Mohsin  
    last_name: Mohammed
    middle_name: Ali
  - first_name: Sai
    last_name: Kandukuri
    middle_name: Teja
  - first_name: Neeharika
    last_name: Gupta
  - first_name: Parth
    last_name: Patwa
  - first_name: Anubhab
    last_name: Chatterjee
  - first_name: Vinija
    last_name: Jain
  - first_name: Aman
    last_name: Chadha
  - first_name: Amitava
    last_name: Das
  file: paper10.pdf
  id: 10
  archival: True
  title: "CONFLATOR: Incorporating Switching Point based Rotatory Positional Encodings 
    for Code-Mixed Language Modeling"
- abstract: Code-Switching (CS) multilingual Automatic Speech Recognition (ASR) models 
    can transcribe speech containing two or more alternating languages during a 
    conversation. This paper proposes (1) a new method for creating code-switching 
    ASR datasets from purely monolingual data sources, and (2) a novel Concatenated 
    Tokenizer that enables ASR models to generate language ID for each emitted text 
    token while reusing existing monolingual tokenizers. The efficacy of these approaches 
    for building CS ASR models is demonstrated for two language pairs, English-Hindi 
    and English-Spanish, where we achieve new state-of-the-art results on the Miami 
    Bangor CS evaluation corpus. In addition to competitive ASR performance, the 
    proposed Concatenated Tokenizer models are highly effective for spoken language 
    identification, achieving 98%+ accuracy on the out-of-distribution FLEURS dataset.
  attributes:
    paper_type: short
    presentation_type: poster
    submitted_area: Area 2
  authors:
  - first_name: Kunal
    last_name: Dhawan
  - first_name: KDimating
    last_name: Rekesh
  - first_name: Boris
    last_name: Ginsburg
  file: paper18.pdf
  id: 18
  archival: True
  title: Unified Model for Code-Switching Speech Recognition and Language Identification 
    Based on Concatenated Tokenizer
- abstract: While many speakers of low-resource languages regularly code-switch 
    between their languages and other regional languages or English, datasets 
    of codeswitched speech are too small to train bespoke acoustic models from 
    scratch or do language model rescoring. Here we propose finetuning 
    self-supervised speech representations such as wav2vec 2.0 XLSR to 
    recognize code-switched data. We find that finetuning self-supervised 
    multilingual representations and augmenting them with n-gram language models 
    trained from transcripts reduces absolute word error rates by up to 20% compared 
    to baselines of hybrid models trained from scratch on code-switched data. Our 
    findings suggest that in circumstances with limited training data finetuning 
    self-supervised representations is a better performing and viable solution.
  attributes:
    paper_type: long
    presentation_type: oral
    submitted_area: Area 1
  authors:
  - first_name: Tolulope
    last_name: Ogunremi
  - first_name: Christopher
    last_name: Manning
  - first_name: Dan
    last_name: Jurafsky
  file: paper20.pdf
  id: 20
  archival: True
  title: Multilingual self-supervised speech representations improve the speech 
    recognition of low-resource African languages with codeswitching